{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea1179c0-b106-4168-92d0-c0e2e6d2c61b",
   "metadata": {},
   "source": [
    "# Comparing Naive Bayes with RNN on the IMDB Dataset\n",
    "\n",
    "In this notebook we load and preprocess the IMDB dataset, then build two classifiers:\n",
    "\n",
    "- A **Naive Bayes** classifier using a binary bag-of-words representation (via `CountVectorizer` and `BernoulliNB`).\n",
    "- An **RNN model** (using the GRU variant with global max pooling) built with PyTorch.\n",
    "\n",
    "We then train both models and compare their performance in terms of accuracy, precision, recall, and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce25bd6e-7922-4b2c-8d81-17f51a13c0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.datasets import imdb\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb1d88f-8f37-4a2b-a4e6-7ad80d05c2b0",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "We load the IMDB dataset (using Keras), convert the tokenized sequences back to text, and split the data into training, validation, and test sets. We then build a custom vocabulary using `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff83d0f-52b5-4fa6-8299-15321b145b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "m = 1000  # Number of words in vocabulary\n",
    "n = 20    # N most frequent words to skip\n",
    "k = 0     # K least frequent words to skip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3a4d54-cd73-4b7d-b8aa-d39f945d6f5d",
   "metadata": {},
   "source": [
    "## Load IMDB Dataset and Create Index-to-Word Mapping\n",
    "\n",
    "Load the dataset using Keras and create a mapping to convert tokenized sequences back to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4a2d05-8ef8-4d8d-86ab-dbb9a4e177e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IMDB dataset\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=m-k, skip_top=n)\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# Create index-to-word mapping\n",
    "index2word = {i + 3: word for word, i in word_index.items()}\n",
    "index2word[0] = '[pad]'\n",
    "index2word[1] = '[bos]'\n",
    "index2word[2] = '[oov]'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8ac99f-7f83-4140-9f24-6c65050c3dca",
   "metadata": {},
   "source": [
    "## Convert Tokenized Sequences Back to Text\n",
    "\n",
    "Transform the numerical sequences into readable text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4b1d9a-5d16-4f95-a7e7-cd26d505a6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [' '.join([index2word.get(idx, '[oov]') for idx in text]) for text in x_train]\n",
    "x_test = [' '.join([index2word.get(idx, '[oov]') for idx in text]) for text in x_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d7ad74-63bd-4ce8-93f3-59398e0ad27a",
   "metadata": {},
   "source": [
    "## Split Training Set into Training and Validation Sets\n",
    "\n",
    "Use scikit-learn to split the training data into training and validation subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0a2e22-bf9d-41b7-b953-67b6cf6a07fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04a9c4a-24a1-4e92-8b0a-2bfc9e6fc430",
   "metadata": {},
   "source": [
    "## Create Custom Vocabulary using CountVectorizer\n",
    "\n",
    "Fit a `CountVectorizer` on the training text to build a vocabulary. Special tokens are added manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42d2d7a-7bb9-45cf-9f27-9a245c99d3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=m, binary=True)\n",
    "vectorizer.fit(X_train)\n",
    "custom_vocab = vectorizer.vocabulary_\n",
    "\n",
    "# Ensure special tokens are in the vocabulary\n",
    "custom_vocab['PAD'] = len(custom_vocab)\n",
    "custom_vocab['UNK'] = len(custom_vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8c6a8a-8a96-4e42-8c6c-2a8e8c45d1ec",
   "metadata": {},
   "source": [
    "## Compute Average Sequence Length\n",
    "\n",
    "Calculate the average length of the tokenized text (after removing non-alphabetical characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36095e7-0f84-41d9-a370-4dd2f6573a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_length = int(np.mean([len(re.sub(r'[^a-zA-Z]', ' ', text.lower()).split()) for text in X_train]))\n",
    "print('Average sequence length:', avg_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb4b2af-2647-4df7-9a14-8c8d2368eae2",
   "metadata": {},
   "source": [
    "## Create Representations\n",
    "\n",
    "Transform the text into a binary bag-of-words representation using the fitted vectorizer. This representation will be used for the Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eec9b8f-bf1e-4a91-8a96-7cfd1bdb5b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_binary = torch.tensor(vectorizer.transform(X_train).toarray(), dtype=torch.float64)\n",
    "X_val_binary = torch.tensor(vectorizer.transform(X_val).toarray(), dtype=torch.float64)\n",
    "X_test_binary = torch.tensor(vectorizer.transform(x_test).toarray(), dtype=torch.float64)\n",
    "\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af1c9d6-2e78-4cbd-8816-88e83e0e0da4",
   "metadata": {},
   "source": [
    "## Create DataLoader for Bag-of-Words Data (Naive Bayes)\n",
    "\n",
    "Wrap the bag-of-words data into PyTorch datasets and create DataLoaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a4b75f-76e7-4e2d-9e68-bd34aebd7a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bag-of-Words representation (for Naive Bayes)\n",
    "X_train_binary = vectorizer.transform(X_train).toarray()\n",
    "X_val_binary = vectorizer.transform(X_val).toarray()\n",
    "X_test_binary = vectorizer.transform(x_test).toarray()\n",
    "\n",
    "# Convert labels to numpy arrays\n",
    "y_train_nb = np.array(y_train)\n",
    "y_val_nb = np.array(y_val)\n",
    "y_test_nb = np.array(y_test)\n",
    "\n",
    "print(f'Training samples: {len(train_dataset)}')\n",
    "print(f'Validation samples: {len(val_dataset)}')\n",
    "print(f'Test samples: {len(test_dataset)}')\n",
    "print(f'Vocabulary size: {len(custom_vocab)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0a3893-224e-4ae4-9c8c-1d0f893e3a8f",
   "metadata": {},
   "source": [
    "## Define Dataset Class for Tokenized Text (for RNN Models)\n",
    "\n",
    "This custom Dataset tokenizes each text using the custom vocabulary and pads/truncates to a fixed maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceda800-8c90-4f6f-92e9-d72617f9292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_length):\n",
    "        self.texts = [self.tokenize(text, vocab, max_length) for text in texts]\n",
    "        self.labels = labels\n",
    "\n",
    "    def tokenize(self, text, vocab, max_length):\n",
    "        text = re.sub(r'[^a-zA-Z]', ' ', text.lower()).split()\n",
    "        tokens = [vocab.get(word, vocab['UNK']) for word in text]\n",
    "        if len(tokens) < max_length:\n",
    "            tokens += [vocab['PAD']] * (max_length - len(tokens))\n",
    "        else:\n",
    "            tokens = tokens[:max_length]\n",
    "        return tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.texts[idx]), torch.tensor(self.labels[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2d76cb-7f82-4b2d-a51f-969cf45ea65f",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "We train a Bernoulli Naive Bayes classifier using the bag-of-words representation and then evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bb2b8b-8d9d-4e4c-a3e3-7253bffb8d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Train Naive Bayes classifier\n",
    "nb = BernoulliNB(alpha=1.0)\n",
    "nb.fit(X_train_binary, y_train_nb)\n",
    "\n",
    "# Evaluate on test set\n",
    "predictions_nb = nb.predict(X_test_binary)\n",
    "acc_nb = accuracy_score(y_test_nb, predictions_nb)\n",
    "prec_nb = precision_score(y_test_nb, predictions_nb)\n",
    "rec_nb = recall_score(y_test_nb, predictions_nb)\n",
    "f1_nb = f1_score(y_test_nb, predictions_nb)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b84b86-4717-4b84-9d2e-3cf17d7c8773",
   "metadata": {},
   "source": [
    "## RNN Model (GRU with Global Max Pooling)\n",
    "\n",
    "We now define an RNN model that uses the GRU variant with global max pooling. The model is trained on the tokenized dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d67d180-8fd3-40b3-bd5d-4c4e0c60cbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim,\n",
    "                 model_type='GRU', use_pooling=True, num_layers=2, bidirectional=True):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.use_pooling = use_pooling\n",
    "        self.bidirectional = bidirectional\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Select the RNN variant\n",
    "        rnn_class = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[model_type]\n",
    "        self.rnn = rnn_class(input_size=embed_dim,\n",
    "                             hidden_size=hidden_dim,\n",
    "                             num_layers=num_layers,\n",
    "                             batch_first=True,\n",
    "                             bidirectional=bidirectional)\n",
    "        \n",
    "        fc_input_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.fc = nn.Linear(fc_input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_length, embed_dim)\n",
    "        output, _ = self.rnn(embedded)  # (batch_size, seq_length, hidden_dim * (2 if bidirectional else 1))\n",
    "        \n",
    "        if self.use_pooling:\n",
    "            pooled = torch.max(output, dim=1)[0]  \n",
    "            return torch.sigmoid(self.fc(pooled))\n",
    "        else:\n",
    "            return torch.sigmoid(self.fc(output[:, -1, :]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a97b90b-4f0c-4686-9b3d-0b8d197c8f73",
   "metadata": {},
   "source": [
    "## Define Training Function\n",
    "\n",
    "Train the model over a given number of epochs and record the training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca9aab7-40a3-49c2-9f26-3f9f87e44c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
    "    train_losses, val_losses = [], []\n",
    "    \n",
    "    model_gru.eval()\n",
    "    running_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in val_loader_rnn:\n",
    "            texts = texts.to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            outputs = model_gru(texts).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item()\n",
    "    avg_val_loss = running_val_loss / len(val_loader_rnn)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7e703b-0e26-4ce7-b1dc-fb8ed201d54f",
   "metadata": {},
   "source": [
    "## Define Evaluation Function\n",
    "\n",
    "# Plot learning curves for the GRU model\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(range(1, epochs+1), train_losses, marker='o', label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), val_losses, marker='o', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('GRU Model Learning Curves')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05d664b-63c8-4ee9-9679-ec4f032a3f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function for RNN models\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in data_loader:\n",
    "            texts = texts.to(device)\n",
    "            preds = (model(texts).squeeze() > 0.5).cpu().tolist()\n",
    "            y_true.extend(labels.tolist())\n",
    "            y_pred.extend(preds)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return accuracy, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b5ad4b-9672-4f14-a484-7b3a04453db6",
   "metadata": {},
   "source": [
    "## Comparing the Two Models\n",
    "\n",
    "Now we compare the evaluation metrics of the Naive Bayes classifier and the GRU model on the test data. The metrics include accuracy, precision, recall, and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e490e89-b6e6-4e5a-b0be-d4d5ce2f64c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison DataFrame\n",
    "comparison_data = {\n",
    "    'Model': ['Naive Bayes', 'GRU'],\n",
    "    'Accuracy': [acc_nb, acc_gru],\n",
    "    'Precision': [prec_nb, prec_gru],\n",
    "    'Recall': [rec_nb, rec_gru],\n",
    "    'F1 Score': [f1_nb, f1_gru]\n",
    "}\n",
    "\n",
    "results = {}\n",
    "epochs = 10\n",
    "\n",
    "for name, model in models.items():\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    print(f\"Training {name}...\")\n",
    "    train_losses, val_losses = train_model(model.float().to(device), train_loader, val_loader,\n",
    "                                           criterion, optimizer, epochs=epochs)\n",
    "    results[name] = {'train_loss': train_losses, 'val_loss': val_losses}\n",
    "    acc, prec, rec, f1 = evaluate_model(model, test_loader)\n",
    "    results[name].update({'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1})\n",
    "\n",
    "# Plot training and validation losses\n",
    "for name in models:\n",
    "    plt.plot(results[name]['train_loss'], linestyle='--', label=f'{name} Train')\n",
    "    plt.plot(results[name]['val_loss'], label=f'{name} Val')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train/Validation Loss Comparison')\n",
    "plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Print evaluation metrics for each model\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name}: Accuracy={metrics['accuracy']:.4f}, Precision={metrics['precision']:.4f}, Recall={metrics['recall']:.4f}, F1={metrics['f1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79e30cf",
   "metadata": {},
   "source": [
    "```plaintext\n",
    "Output:\n",
    "cpu\n",
    "cpu\n",
    "Training samples: 20000\n",
    "Validation samples: 5000\n",
    "Test samples: 25000     \n",
    "Vocabulary size: 955    \n",
    "Training RNN...\n",
    "Epoch:    0 / 10 | Training Loss: 164.45803, Validation Loss: 32.73606\n",
    "Epoch:    2 / 10 | Training Loss: 102.47885, Validation Loss: 28.72308\n",
    "Epoch:    4 / 10 | Training Loss: 76.53272, Validation Loss: 30.31154\n",
    "Epoch:    6 / 10 | Training Loss: 51.84893, Validation Loss: 31.30217\n",
    "Epoch:    8 / 10 | Training Loss: 29.06599, Validation Loss: 35.54264\n",
    "Training GRU...\n",
    "Epoch:    0 / 10 | Training Loss: 155.78530, Validation Loss: 30.41802\n",
    "Epoch:    2 / 10 | Training Loss: 81.56101, Validation Loss: 28.66211\n",
    "Epoch:    4 / 10 | Training Loss: 42.58857, Validation Loss: 38.59015\n",
    "Epoch:    6 / 10 | Training Loss: 11.54743, Validation Loss: 56.28524\n",
    "Epoch:    8 / 10 | Training Loss: 5.88370, Validation Loss: 69.25617\n",
    "Training LSTM...\n",
    "Epoch:    0 / 10 | Training Loss: 155.47545, Validation Loss: 32.04157\n",
    "Epoch:    2 / 10 | Training Loss: 77.85037, Validation Loss: 28.94105\n",
    "Epoch:    4 / 10 | Training Loss: 36.44361, Validation Loss: 39.11787\n",
    "Epoch:    6 / 10 | Training Loss: 14.38555, Validation Loss: 48.93268\n",
    "Epoch:    8 / 10 | Training Loss: 7.20283, Validation Loss: 59.62413\n",
    "RNN: Accuracy=0.8348, Precision=0.8357, Recall=0.8335, F1=0.8346\n",
    "GRU: Accuracy=0.8421, Precision=0.8493, Recall=0.8317, F1=0.8404\n",
    "LSTM: Accuracy=0.8394, Precision=0.8201, Recall=0.8696, F1=0.8441\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31a4351",
   "metadata": {},
   "source": [
    "![](AI_Project_IMDB/NaiveBayes/RNN/RNN_Curves.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d94adf5-0872-4cc4-8f3f-11a85a99aef4",
   "metadata": {},
   "source": [
    "## End of Comparison\n",
    "\n",
    "The table above shows the performance of both the Naive Bayes classifier and the GRU-based RNN on the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
