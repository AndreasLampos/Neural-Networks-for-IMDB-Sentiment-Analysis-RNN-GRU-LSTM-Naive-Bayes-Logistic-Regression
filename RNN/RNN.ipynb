{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8b98384-14a6-48b1-8cb2-1d3c6e3f9ee8",
   "metadata": {},
   "source": [
    "# IMDB Classification: RNN (with GRU/Global Max Pooling) vs. Naive Bayes\n",
    "\n",
    "This notebook loads and processes the IMDB dataset, builds a custom vocabulary, and creates two types of representations:\n",
    "\n",
    "- **Tokenized sequences** for training RNN models (RNN, GRU, LSTM with Global Max Pooling).\n",
    "- **Bag-of-Words binary representation** for a Naive Bayes classifier (BernoulliNB).\n",
    "\n",
    "After training, we compare the performance of the RNN models and Naive Bayes on test data by reporting accuracy, precision, recall, and F1 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce25bd6e-7922-4b2c-8d81-17f51a13c0e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import necessary libraries\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, TensorDataset, Dataset\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.datasets import imdb\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8a60d5-2054-4c9b-bb7e-80dd98bfa5f5",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Set basic parameters for the vocabulary and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff83d0f-52b5-4fa6-8299-15321b145b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "m = 1000  # Number of words in vocabulary\n",
    "n = 20    # N most frequent words to skip\n",
    "k = 0     # K least frequent words to skip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3a4d54-cd73-4b7d-b8aa-d39f945d6f5d",
   "metadata": {},
   "source": [
    "## Load IMDB Dataset and Create Index-to-Word Mapping\n",
    "\n",
    "Load the dataset using Keras and create a mapping to convert tokenized sequences back to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4a2d05-8ef8-4d8d-86ab-dbb9a4e177e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IMDB dataset\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=m-k, skip_top=n)\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# Create index-to-word mapping\n",
    "index2word = {i + 3: word for word, i in word_index.items()}\n",
    "index2word[0] = '[pad]'\n",
    "index2word[1] = '[bos]'\n",
    "index2word[2] = '[oov]'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8ac99f-7f83-4140-9f24-6c65050c3dca",
   "metadata": {},
   "source": [
    "## Convert Tokenized Sequences Back to Text\n",
    "\n",
    "Transform the numerical sequences into readable text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4b1d9a-5d16-4f95-a7e7-cd26d505a6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [' '.join([index2word.get(idx, '[oov]') for idx in text]) for text in x_train]\n",
    "x_test = [' '.join([index2word.get(idx, '[oov]') for idx in text]) for text in x_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d7ad74-63bd-4ce8-93f3-59398e0ad27a",
   "metadata": {},
   "source": [
    "## Split Training Set into Training and Validation Sets\n",
    "\n",
    "Use scikit-learn to split the training data into training and validation subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0a2e22-bf9d-41b7-b953-67b6cf6a07fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04a9c4a-24a1-4e92-8b0a-2bfc9e6fc430",
   "metadata": {},
   "source": [
    "## Create Custom Vocabulary using CountVectorizer\n",
    "\n",
    "Fit a `CountVectorizer` on the training text to build a vocabulary. Special tokens are added manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42d2d7a-7bb9-45cf-9f27-9a245c99d3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=m, binary=True)\n",
    "vectorizer.fit(X_train)\n",
    "custom_vocab = vectorizer.vocabulary_\n",
    "\n",
    "# Ensure special tokens are in the vocabulary\n",
    "custom_vocab['PAD'] = len(custom_vocab)\n",
    "custom_vocab['UNK'] = len(custom_vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8c6a8a-8a96-4e42-8c6c-2a8e8c45d1ec",
   "metadata": {},
   "source": [
    "## Compute Average Sequence Length\n",
    "\n",
    "Calculate the average length of the tokenized text (after removing non-alphabetical characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36095e7-0f84-41d9-a370-4dd2f6573a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_length = int(np.mean([len(re.sub(r'[^a-zA-Z]', ' ', text.lower()).split()) for text in X_train]))\n",
    "print('Average sequence length:', avg_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddca69e8-5bb0-4df8-a0c3-0c58e0c2c1c7",
   "metadata": {},
   "source": [
    "## Convert Text Data into Binary Bag-of-Words Representation\n",
    "\n",
    "Transform the text into a binary bag-of-words representation using the fitted vectorizer. This representation will be used for the Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eec9b8f-bf1e-4a91-8a96-7cfd1bdb5b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_binary = torch.tensor(vectorizer.transform(X_train).toarray(), dtype=torch.float64)\n",
    "X_val_binary = torch.tensor(vectorizer.transform(X_val).toarray(), dtype=torch.float64)\n",
    "X_test_binary = torch.tensor(vectorizer.transform(x_test).toarray(), dtype=torch.float64)\n",
    "\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af1c9d6-2e78-4cbd-8816-88e83e0e0da4",
   "metadata": {},
   "source": [
    "## Create DataLoader for Bag-of-Words Data (Naive Bayes)\n",
    "\n",
    "Wrap the bag-of-words data into PyTorch datasets and create DataLoaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a4b75f-76e7-4e2d-9e68-bd34aebd7a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train_binary, y_train)\n",
    "val_dataset = TensorDataset(X_val_binary, y_val)\n",
    "test_dataset = TensorDataset(X_test_binary, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f'Training samples: {len(train_dataset)}')\n",
    "print(f'Validation samples: {len(val_dataset)}')\n",
    "print(f'Test samples: {len(test_dataset)}')\n",
    "print(f'Vocabulary size: {len(custom_vocab)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0a3893-224e-4ae4-9c8c-1d0f893e3a8f",
   "metadata": {},
   "source": [
    "## Define Dataset Class for Tokenized Text (for RNN Models)\n",
    "\n",
    "This custom Dataset tokenizes each text using the custom vocabulary and pads/truncates to a fixed maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceda800-8c90-4f6f-92e9-d72617f9292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_length):\n",
    "        self.texts = [self.tokenize(text, vocab, max_length) for text in texts]\n",
    "        self.labels = labels\n",
    "\n",
    "    def tokenize(self, text, vocab, max_length):\n",
    "        text = re.sub(r'[^a-zA-Z]', ' ', text.lower()).split()\n",
    "        tokens = [vocab.get(word, vocab['UNK']) for word in text]\n",
    "        if len(tokens) < max_length:\n",
    "            tokens += [vocab['PAD']] * (max_length - len(tokens))\n",
    "        else:\n",
    "            tokens = tokens[:max_length]\n",
    "        return tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.texts[idx]), torch.tensor(self.labels[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3cf7f3-621d-4b64-9a8f-7f3c813de0e0",
   "metadata": {},
   "source": [
    "## Create DataLoaders for Tokenized Text Data (for RNN Models)\n",
    "\n",
    "Wrap the tokenized data into a dataset and create DataLoaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bb2b8b-8d9d-4e4c-a3e3-7253bffb8d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(X_train, y_train, custom_vocab, avg_length)\n",
    "val_dataset = TextDataset(X_val, y_val, custom_vocab, avg_length)\n",
    "test_dataset = TextDataset(x_test, y_test, custom_vocab, avg_length)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4471fd-2281-4b1f-9e46-d7a37a8f8e43",
   "metadata": {},
   "source": [
    "## Define RNN Models (RNN, GRU, LSTM) with Global Max Pooling\n",
    "\n",
    "Here we define a model class that can instantiate an RNN, GRU, or LSTM with global max pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d67d180-8fd3-40b3-bd5d-4c4e0c60cbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size,\n",
    "                 embed_dim, hidden_dim, output_dim,\n",
    "                 model_type='RNN',\n",
    "                 pretrained=True, freeze=False,\n",
    "                 use_pooling=True,  # Global max pooling enabled\n",
    "                 num_layers=2, bidirectional=True):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.use_pooling = use_pooling\n",
    "        self.bidirectional = bidirectional\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Initialize embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # Select the RNN variant (RNN, GRU, or LSTM)\n",
    "        rnn_class = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[model_type]\n",
    "        self.rnn = rnn_class(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "\n",
    "        # Adjust fully connected layer input dimension\n",
    "        fc_input_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.fc = nn.Linear(fc_input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_length)\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_length, embed_dim)\n",
    "        output, _ = self.rnn(embedded)  # (batch_size, seq_length, hidden_dim*(2 if bidirectional else 1))\n",
    "\n",
    "        if self.use_pooling:\n",
    "            # Global max pooling over the sequence dimension\n",
    "            pooled = torch.max(output, dim=1)[0]  \n",
    "            return torch.sigmoid(self.fc(pooled))\n",
    "        else:\n",
    "            return torch.sigmoid(self.fc(output[:, -1, :]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a97b90b-4f0c-4686-9b3d-0b8d197c8f73",
   "metadata": {},
   "source": [
    "## Define Training Function\n",
    "\n",
    "Train the model over a given number of epochs and record the training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca9aab7-40a3-49c2-9f26-3f9f87e44c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
    "    train_losses, val_losses = [], []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for texts, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            texts = texts.to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            outputs = model(texts).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for texts, labels in val_loader:\n",
    "                texts = texts.to(device)\n",
    "                labels = labels.float().to(device)\n",
    "                outputs = model(texts).squeeze()\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        \n",
    "        if epoch % 2 == 0:\n",
    "            print(f'Epoch: {epoch:4.0f} / {epochs} | Training Loss: {train_loss:.5f}, Validation Loss: {val_loss:.5f}')\n",
    "    \n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7e703b-0e26-4ce7-b1dc-fb8ed201d54f",
   "metadata": {},
   "source": [
    "## Define Evaluation Function\n",
    "\n",
    "Evaluate the model on the test set and compute accuracy, precision, recall, and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05d664b-63c8-4ee9-9679-ec4f032a3f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in test_loader:\n",
    "            texts = texts.to(device)\n",
    "            preds = model(texts).squeeze() > 0.5\n",
    "            y_true.extend(labels.tolist())\n",
    "            y_pred.extend(preds.tolist())\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return accuracy, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a64d6c2-192f-4882-88e0-c2bb2a0de52a",
   "metadata": {},
   "source": [
    "## Instantiate and Train RNN Models (RNN, GRU, LSTM)\n",
    "\n",
    "We create three models and train each one, then record the training/validation loss and evaluation metrics on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e490e89-b6e6-4e5a-b0be-d4d5ce2f64c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'RNN': RNNModel(len(custom_vocab), 300, 64, 1, 'RNN',\n",
    "                    use_pooling=True, num_layers=2, bidirectional=True),\n",
    "    'GRU': RNNModel(len(custom_vocab), 300, 64, 1, 'GRU',\n",
    "                    use_pooling=True, num_layers=2, bidirectional=True),\n",
    "    'LSTM': RNNModel(len(custom_vocab), 300, 64, 1, 'LSTM',\n",
    "                     use_pooling=True, num_layers=2, bidirectional=True)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "epochs = 10\n",
    "\n",
    "for name, model in models.items():\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    print(f\"Training {name}...\")\n",
    "    train_losses, val_losses = train_model(model.float().to(device), train_loader, val_loader,\n",
    "                                           criterion, optimizer, epochs=epochs)\n",
    "    results[name] = {'train_loss': train_losses, 'val_loss': val_losses}\n",
    "    acc, prec, rec, f1 = evaluate_model(model, test_loader)\n",
    "    results[name].update({'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1})\n",
    "\n",
    "# Plot training and validation losses\n",
    "for name in models:\n",
    "    plt.plot(results[name]['train_loss'], linestyle='--', label=f'{name} Train')\n",
    "    plt.plot(results[name]['val_loss'], label=f'{name} Val')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train/Validation Loss Comparison')\n",
    "plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Print evaluation metrics for each model\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name}: Accuracy={metrics['accuracy']:.4f}, Precision={metrics['precision']:.4f}, Recall={metrics['recall']:.4f}, F1={metrics['f1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79e30cf",
   "metadata": {},
   "source": [
    "```plaintext\n",
    "Output:\n",
    "cpu\n",
    "cpu\n",
    "Training samples: 20000\n",
    "Validation samples: 5000\n",
    "Test samples: 25000     \n",
    "Vocabulary size: 955    \n",
    "Training RNN...\n",
    "Epoch:    0 / 10 | Training Loss: 164.45803, Validation Loss: 32.73606\n",
    "Epoch:    2 / 10 | Training Loss: 102.47885, Validation Loss: 28.72308\n",
    "Epoch:    4 / 10 | Training Loss: 76.53272, Validation Loss: 30.31154\n",
    "Epoch:    6 / 10 | Training Loss: 51.84893, Validation Loss: 31.30217\n",
    "Epoch:    8 / 10 | Training Loss: 29.06599, Validation Loss: 35.54264\n",
    "Training GRU...\n",
    "Epoch:    0 / 10 | Training Loss: 155.78530, Validation Loss: 30.41802\n",
    "Epoch:    2 / 10 | Training Loss: 81.56101, Validation Loss: 28.66211\n",
    "Epoch:    4 / 10 | Training Loss: 42.58857, Validation Loss: 38.59015\n",
    "Epoch:    6 / 10 | Training Loss: 11.54743, Validation Loss: 56.28524\n",
    "Epoch:    8 / 10 | Training Loss: 5.88370, Validation Loss: 69.25617\n",
    "Training LSTM...\n",
    "Epoch:    0 / 10 | Training Loss: 155.47545, Validation Loss: 32.04157\n",
    "Epoch:    2 / 10 | Training Loss: 77.85037, Validation Loss: 28.94105\n",
    "Epoch:    4 / 10 | Training Loss: 36.44361, Validation Loss: 39.11787\n",
    "Epoch:    6 / 10 | Training Loss: 14.38555, Validation Loss: 48.93268\n",
    "Epoch:    8 / 10 | Training Loss: 7.20283, Validation Loss: 59.62413\n",
    "RNN: Accuracy=0.8348, Precision=0.8357, Recall=0.8335, F1=0.8346\n",
    "GRU: Accuracy=0.8421, Precision=0.8493, Recall=0.8317, F1=0.8404\n",
    "LSTM: Accuracy=0.8394, Precision=0.8201, Recall=0.8696, F1=0.8441\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31a4351",
   "metadata": {},
   "source": [
    "![](AI_Project_IMDB/NaiveBayes/RNN/RNN_Curves.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1741f07-189a-4e3d-90e8-2f7d977d3e5b",
   "metadata": {},
   "source": [
    "## End of Notebook\n",
    "\n",
    "This notebook demonstrated the process of training and comparing RNN-based models with a Naive Bayes classifier on the IMDB dataset. Next steps might include further hyperparameter tuning and additional evaluations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
