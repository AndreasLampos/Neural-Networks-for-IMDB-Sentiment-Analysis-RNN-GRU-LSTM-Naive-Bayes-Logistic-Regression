{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea1179c0-b106-4168-92d0-c0e2e6d2c61b",
   "metadata": {},
   "source": [
    "# Comparing Naive Bayes with RNN on the IMDB Dataset\n",
    "\n",
    "In this notebook we load and preprocess the IMDB dataset, then build two classifiers:\n",
    "\n",
    "- A **Naive Bayes** classifier using a binary bag-of-words representation (via `CountVectorizer` and `BernoulliNB`).\n",
    "- An **RNN model** (using the GRU variant with global max pooling) built with PyTorch.\n",
    "\n",
    "We then train both models and compare their performance in terms of accuracy, precision, recall, and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "id": "ce296391-0bb4-40f3-9f21-1b7c05e5c391",
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.datasets import imdb\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('Using device:', device)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fdb1d88f-8f37-4a2b-a4e6-7ad80d05c2b0",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "We load the IMDB dataset (using Keras), convert the tokenized sequences back to text, and split the data into training, validation, and test sets. We then build a custom vocabulary using `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "id": "0e3eeb0a-b7d2-4c89-b8f2-c4cbbd4d41ad",
   "metadata": {},
   "source": [
    "# Parameters for vocabulary\n",
    "m = 1000  # number of words in vocabulary\n",
    "n = 20    # skip top 20 most frequent words\n",
    "k = 0     # skip 0 least frequent words\n",
    "\n",
    "# Load IMDB dataset\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=m-k, skip_top=n)\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# Create index-to-word mapping\n",
    "index2word = {i + 3: word for word, i in word_index.items()}\n",
    "index2word[0] = '[pad]'\n",
    "index2word[1] = '[bos]'\n",
    "index2word[2] = '[oov]'\n",
    "\n",
    "# Convert tokenized sequences back to text\n",
    "x_train = [' '.join([index2word.get(idx, '[oov]') for idx in text]) for text in x_train]\n",
    "x_test = [' '.join([index2word.get(idx, '[oov]') for idx in text]) for text in x_test]\n",
    "\n",
    "# Split training set into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create custom vocabulary using CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=m, binary=True)\n",
    "vectorizer.fit(X_train)\n",
    "custom_vocab = vectorizer.vocabulary_\n",
    "\n",
    "# Ensure special tokens are in the vocabulary\n",
    "custom_vocab['PAD'] = len(custom_vocab)\n",
    "custom_vocab['UNK'] = len(custom_vocab)\n",
    "\n",
    "# Compute average sequence length\n",
    "avg_length = int(np.mean([len(re.sub(r'[^a-zA-Z]', ' ', text.lower()).split()) for text in X_train]))\n",
    "print('Average sequence length:', avg_length)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "efb4b2af-2647-4df7-9a14-8c8d2368eae2",
   "metadata": {},
   "source": [
    "## Create Representations\n",
    "\n",
    "We prepare two representations:\n",
    "\n",
    "1. **Bag-of-Words representation** for the Naive Bayes classifier.\n",
    "2. **Tokenized and padded sequences** for the RNN models."
   ]
  },
  {
   "cell_type": "code",
   "id": "8d7c5a8e-0ea7-41da-95ec-7f1c06a8a633",
   "metadata": {},
   "source": [
    "# Create Bag-of-Words representation (for Naive Bayes)\n",
    "X_train_binary = vectorizer.transform(X_train).toarray()\n",
    "X_val_binary = vectorizer.transform(X_val).toarray()\n",
    "X_test_binary = vectorizer.transform(x_test).toarray()\n",
    "\n",
    "# Convert labels to numpy arrays\n",
    "y_train_nb = np.array(y_train)\n",
    "y_val_nb = np.array(y_val)\n",
    "y_test_nb = np.array(y_test)\n",
    "\n",
    "print(f'Training samples: {len(X_train_binary)}')\n",
    "print(f'Validation samples: {len(X_val_binary)}')\n",
    "print(f'Test samples: {len(X_test_binary)}')\n",
    "print(f'Vocabulary size: {len(custom_vocab)}')\n",
    "\n",
    "# Define a custom Dataset for tokenized text (for RNN models)\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_length):\n",
    "        self.texts = [self.tokenize(text, vocab, max_length) for text in texts]\n",
    "        self.labels = labels\n",
    "\n",
    "    def tokenize(self, text, vocab, max_length):\n",
    "        text = re.sub(r'[^a-zA-Z]', ' ', text.lower()).split()\n",
    "        tokens = [vocab.get(word, vocab['UNK']) for word in text]\n",
    "        if len(tokens) < max_length:\n",
    "            tokens += [vocab['PAD']] * (max_length - len(tokens))\n",
    "        else:\n",
    "            tokens = tokens[:max_length]\n",
    "        return tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.texts[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "# Create Datasets and DataLoaders for RNN models\n",
    "train_dataset_rnn = TextDataset(X_train, y_train, custom_vocab, avg_length)\n",
    "val_dataset_rnn = TextDataset(X_val, y_val, custom_vocab, avg_length)\n",
    "test_dataset_rnn = TextDataset(x_test, y_test, custom_vocab, avg_length)\n",
    "\n",
    "train_loader_rnn = DataLoader(train_dataset_rnn, batch_size=64, shuffle=True)\n",
    "val_loader_rnn = DataLoader(val_dataset_rnn, batch_size=64, shuffle=False)\n",
    "test_loader_rnn = DataLoader(test_dataset_rnn, batch_size=64, shuffle=False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ea2d76cb-7f82-4b2d-a51f-969cf45ea65f",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "We train a Bernoulli Naive Bayes classifier using the bag-of-words representation and then evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "id": "5e87dbdc-9d27-4b1e-b81a-6d123a9710bd",
   "metadata": {},
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Train Naive Bayes classifier\n",
    "nb = BernoulliNB(alpha=1.0)\n",
    "nb.fit(X_train_binary, y_train_nb)\n",
    "\n",
    "# Evaluate on test set\n",
    "predictions_nb = nb.predict(X_test_binary)\n",
    "acc_nb = accuracy_score(y_test_nb, predictions_nb)\n",
    "prec_nb = precision_score(y_test_nb, predictions_nb)\n",
    "rec_nb = recall_score(y_test_nb, predictions_nb)\n",
    "f1_nb = f1_score(y_test_nb, predictions_nb)\n",
    "\n",
    "print(f\"Naive Bayes: Accuracy={acc_nb:.4f}, Precision={prec_nb:.4f}, Recall={rec_nb:.4f}, F1={f1_nb:.4f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "84b84b86-4717-4b84-9d2e-3cf17d7c8773",
   "metadata": {},
   "source": [
    "## RNN Model (GRU with Global Max Pooling)\n",
    "\n",
    "We now define an RNN model that uses the GRU variant with global max pooling. The model is trained on the tokenized dataset."
   ]
  },
  {
   "cell_type": "code",
   "id": "43b3efbb-9c4c-46f1-a2b4-9c6fd5a3eec8",
   "metadata": {},
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim,\n",
    "                 model_type='GRU', use_pooling=True, num_layers=2, bidirectional=True):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.use_pooling = use_pooling\n",
    "        self.bidirectional = bidirectional\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Select the RNN variant\n",
    "        rnn_class = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[model_type]\n",
    "        self.rnn = rnn_class(input_size=embed_dim,\n",
    "                             hidden_size=hidden_dim,\n",
    "                             num_layers=num_layers,\n",
    "                             batch_first=True,\n",
    "                             bidirectional=bidirectional)\n",
    "        \n",
    "        fc_input_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.fc = nn.Linear(fc_input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_length, embed_dim)\n",
    "        output, _ = self.rnn(embedded)  # (batch_size, seq_length, hidden_dim * (2 if bidirectional else 1))\n",
    "        \n",
    "        if self.use_pooling:\n",
    "            pooled = torch.max(output, dim=1)[0]  \n",
    "            return torch.sigmoid(self.fc(pooled))\n",
    "        else:\n",
    "            return torch.sigmoid(self.fc(output[:, -1, :]))\n",
    "\n",
    "# Instantiate and train the GRU model\n",
    "vocab_size = len(custom_vocab) + 1  # +1 for potential PAD token\n",
    "embed_dim = 300\n",
    "hidden_dim = 64\n",
    "output_dim = 1\n",
    "\n",
    "model_gru = RNNModel(vocab_size, embed_dim, hidden_dim, output_dim, model_type='GRU', use_pooling=True, num_layers=2, bidirectional=True)\n",
    "model_gru = model_gru.float().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model_gru.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Training GRU model...\")\n",
    "epochs = 10\n",
    "train_losses, val_losses = [], []\n",
    "for epoch in range(epochs):\n",
    "    model_gru.train()\n",
    "    running_train_loss = 0\n",
    "    for texts, labels in train_loader_rnn:\n",
    "        optimizer.zero_grad()\n",
    "        texts = texts.to(device)\n",
    "        labels = labels.float().to(device)\n",
    "        outputs = model_gru(texts).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += loss.item()\n",
    "    avg_train_loss = running_train_loss / len(train_loader_rnn)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    model_gru.eval()\n",
    "    running_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in val_loader_rnn:\n",
    "            texts = texts.to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            outputs = model_gru(texts).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item()\n",
    "    avg_val_loss = running_val_loss / len(val_loader_rnn)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        print(f\"Epoch: {epoch:4d} / {epochs} | Train Loss: {avg_train_loss:.5f}, Val Loss: {avg_val_loss:.5f}\")\n",
    "\n",
    "# Plot learning curves for the GRU model\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(range(1, epochs+1), train_losses, marker='o', label='Train Loss')\n",
    "plt.plot(range(1, epochs+1), val_losses, marker='o', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('GRU Model Learning Curves')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "30bdf6ea-d7a8-4a5e-95fc-8ab1caa9e84e",
   "metadata": {},
   "source": [
    "# Evaluation function for RNN models\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in data_loader:\n",
    "            texts = texts.to(device)\n",
    "            preds = (model(texts).squeeze() > 0.5).cpu().tolist()\n",
    "            y_true.extend(labels.tolist())\n",
    "            y_pred.extend(preds)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "# Evaluate the GRU model on test data\n",
    "acc_gru, prec_gru, rec_gru, f1_gru = evaluate_model(model_gru, test_loader_rnn)\n",
    "print(f\"GRU: Accuracy={acc_gru:.4f}, Precision={prec_gru:.4f}, Recall={rec_gru:.4f}, F1={f1_gru:.4f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f8b5ad4b-9672-4f14-a484-7b3a04453db6",
   "metadata": {},
   "source": [
    "## Comparing the Two Models\n",
    "\n",
    "Now we compare the evaluation metrics of the Naive Bayes classifier and the GRU model on the test data. The metrics include accuracy, precision, recall, and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "id": "8c2c2679-c14b-4a31-89c2-f0764e8f6fd1",
   "metadata": {},
   "source": [
    "# Create a comparison DataFrame\n",
    "comparison_data = {\n",
    "    'Model': ['Naive Bayes', 'GRU'],\n",
    "    'Accuracy': [acc_nb, acc_gru],\n",
    "    'Precision': [prec_nb, prec_gru],\n",
    "    'Recall': [rec_nb, rec_gru],\n",
    "    'F1 Score': [f1_nb, f1_gru]\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(df_comparison.to_string(index=False))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7d94adf5-0872-4cc4-8f3f-11a85a99aef4",
   "metadata": {},
   "source": [
    "## End of Comparison\n",
    "\n",
    "The table above shows the performance of both the Naive Bayes classifier and the GRU-based RNN on the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
