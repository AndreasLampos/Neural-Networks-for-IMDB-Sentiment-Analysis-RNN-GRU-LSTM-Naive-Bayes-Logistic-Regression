{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07b6ac3-7759-4a1c-a02d-b1e2184e8ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "# from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from torchtext.data.utils import get_tokenizer\n",
    "from tensorflow.keras.datasets import imdb\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "# import gensim.downloader as api\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "\n",
    "# Parameters\n",
    "m = 1000  # Number of words in vocabulary\n",
    "n = 20   # N most frequent words to skipdevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "k = 0     # K least frequent words to skip\n",
    "\n",
    "# Load IMDB dataset\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=m-k, skip_top=n)\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# Create index-to-word mapping\n",
    "index2word = {i + 3: word for word, i in word_index.items()}\n",
    "index2word[0] = '[pad]'\n",
    "index2word[1] = '[bos]'\n",
    "index2word[2] = '[oov]'\n",
    "\n",
    "# Convert tokenized sequences back to text\n",
    "x_train = [' '.join([index2word.get(idx, '[oov]') for idx in text]) for text in x_train]\n",
    "x_test = [' '.join([index2word.get(idx, '[oov]') for idx in text]) for text in x_test]\n",
    "\n",
    "# Split train set further into train/validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create custom vocabulary using CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=m, binary=True)\n",
    "vectorizer.fit(X_train)\n",
    "custom_vocab = vectorizer.vocabulary_\n",
    "\n",
    "# Ensure special tokens are in the vocabulary\n",
    "custom_vocab['PAD'] = len(custom_vocab)\n",
    "custom_vocab['UNK'] = len(custom_vocab)\n",
    "\n",
    "# Compute average sequence length\n",
    "avg_length = int(np.mean([len(re.sub(r'[^a-zA-Z]', ' ', text.lower()).split()) for text in X_train]))\n",
    "\n",
    "# Convert text data into binary bag-of-words representation\n",
    "X_train_binary = torch.tensor(vectorizer.transform(X_train).toarray(), dtype=torch.float64)\n",
    "X_val_binary = torch.tensor(vectorizer.transform(X_val).toarray(), dtype=torch.float64)\n",
    "X_test_binary = torch.tensor(vectorizer.transform(x_test).toarray(), dtype=torch.float64)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader for PyTorch\n",
    "train_dataset = TensorDataset(X_train_binary, y_train)\n",
    "val_dataset = TensorDataset(X_val_binary, y_val)\n",
    "test_dataset = TensorDataset(X_test_binary, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Print dataset info\n",
    "print(f'Training samples: {len(train_dataset)}')\n",
    "print(f'Validation samples: {len(val_dataset)}')\n",
    "print(f'Test samples: {len(test_dataset)}')\n",
    "print(f'Vocabulary size: {len(custom_vocab)}')\n",
    "\n",
    "\n",
    "\n",
    "# Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_length):\n",
    "        self.texts = [self.tokenize(text, vocab, max_length) for text in texts]\n",
    "        self.labels = labels\n",
    "\n",
    "    def tokenize(self, text, vocab, max_length):\n",
    "        text = re.sub(r'[^a-zA-Z]', ' ', text.lower()).split()\n",
    "        tokens = [vocab.get(word, vocab['UNK']) for word in text]\n",
    "        if len(tokens) < max_length:\n",
    "            tokens += [vocab['PAD']] * (max_length - len(tokens))\n",
    "        else:\n",
    "            tokens = tokens[:max_length]\n",
    "        return tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.texts[idx]), torch.tensor(self.labels[idx])\n",
    "\n",
    "train_dataset = TextDataset(X_train, y_train, custom_vocab, avg_length)\n",
    "val_dataset = TextDataset(X_val, y_val, custom_vocab, avg_length)\n",
    "test_dataset = TextDataset(x_test, y_test, custom_vocab, avg_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "\n",
    "\n",
    "# Define RNN, GRU, and LSTM models with optional Global Max Pooling\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size,\n",
    "                 embed_dim, hidden_dim, output_dim,\n",
    "                 model_type='RNN',\n",
    "                 pretrained=True, freeze=False,\n",
    "                 use_pooling=True,  # Global max pooling is enabled\n",
    "                 num_layers=2, bidirectional=True):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.use_pooling = use_pooling\n",
    "        self.bidirectional = bidirectional\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Initialize embeddings (pretrained handling can be added if needed)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # Select the RNN variant (RNN, GRU, or LSTM)\n",
    "        rnn_class = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[model_type]\n",
    "        self.rnn = rnn_class(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "\n",
    "        # Adjust the input dimension for the fully connected layer if bidirectional\n",
    "        fc_input_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.fc = nn.Linear(fc_input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_length)\n",
    "        embedded = self.embedding(x)  # shape: (batch_size, seq_length, embed_dim)\n",
    "        output, _ = self.rnn(embedded)  # output shape: (batch_size, seq_length, hidden_dim*(2 if bidirectional else 1))\n",
    "\n",
    "        if self.use_pooling:\n",
    "            # Global max pooling over the sequence length dimension\n",
    "            pooled = torch.max(output, dim=1)[0]  # shape: (batch_size, hidden_dim*(2 if bidirectional else 1))\n",
    "            return torch.sigmoid(self.fc(pooled))\n",
    "        else:\n",
    "            # If not using pooling, one might use the last time step (not used here)\n",
    "            return torch.sigmoid(self.fc(output[:, -1, :]))\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        # Training and Evaluation Functions remain unchanged\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for texts, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            texts = texts.to(device)\n",
    "            labels = labels.float().to(device)\n",
    "            outputs = model(texts).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for texts, labels in val_loader:\n",
    "                texts = texts.to(device)\n",
    "                labels = labels.float().to(device)\n",
    "                outputs = model(texts).squeeze()\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "\n",
    "        if epoch % 2 == 0:\n",
    "            print(f'Epoch: {epoch:4.0f} / {epochs} | Training Loss: {train_loss:.5f}, Validation Loss: {val_loss:.5f}')\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in test_loader:\n",
    "            texts = texts.to(device)\n",
    "            preds = model(texts).squeeze() > 0.5\n",
    "            y_true.extend(labels.tolist())\n",
    "            y_pred.extend(preds.tolist())\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "# Instantiate models and train\n",
    "models = {\n",
    "    'RNN': RNNModel(len(custom_vocab), 300, 64, 1, 'RNN',\n",
    "                    use_pooling=True, num_layers=2, bidirectional=True),\n",
    "    'GRU': RNNModel(len(custom_vocab), 300, 64, 1, 'GRU',\n",
    "                    use_pooling=True, num_layers=2, bidirectional=True),\n",
    "    'LSTM': RNNModel(len(custom_vocab), 300, 64, 1, 'LSTM',\n",
    "                     use_pooling=True, num_layers=2, bidirectional=True)\n",
    "}\n",
    "    \n",
    "\n",
    "results = {}\n",
    "epochs = 10\n",
    "\n",
    "for name, model in models.items():\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    print(f\"Training {name}...\")\n",
    "    train_losses, val_losses = train_model(model.float().to(device), train_loader, val_loader,\n",
    "                                           criterion, optimizer, epochs=epochs)\n",
    "    results[name] = {'train_loss': train_losses, 'val_loss': val_losses}\n",
    "    acc, prec, rec, f1 = evaluate_model(model, test_loader)\n",
    "    results[name].update({'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1})\n",
    "\n",
    "# Plot losses\n",
    "for name in models:\n",
    "    plt.plot(results[name]['train_loss'], linestyle='--', label=f'{name} Train')\n",
    "    plt.plot(results[name]['val_loss'], label=f'{name} Val')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Train/Validation Loss Comparison')\n",
    "plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Print evaluation metrics\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name}: Accuracy={metrics['accuracy']:.4f}, Precision={metrics['precision']:.4f}, Recall={metrics['recall']:.4f}, F1={metrics['f1']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
